seq2seq:
    num_samples: 1000000
    train_ratio: 0.8
    val_ratio: 0.1
    max_encoder_seq_length: 30
    max_decoder_seq_length: 30
    encoder_emb_dim: 256
    decoder_emb_dim: 256
    hidden_dim: 512
    n_layers: 2
    encoder_dropout: 0.3
    decoder_dropout: 0.3
    n_epochs: 300
    patience: 10
    batch_size: 64
    use_attention: True
    variable_lengths: True
    model_path: state_dict/g2p_english.pt
    ;model_path: state_dict/norm_asr_v2.pt
    ;model_path: addtone.pt
    ;data_path: ../resources/foreign-lexicon-13k_one_type.lex
    data_path: ../resources/seq2seq/g2p_english.csv
    ;data_path: ../resources/foreign.csv
    ;data_path: ../resources/multi_pronunciation.txt
    ;data_path: ../resources/train_tokenizer.csv
    ;data_path: ../text-normalization-challenge-english-language/shard_0000_token.csv
    ;data_pattern: /media/nghidinh/DATA/data/text_corpus/News_corpus_VuQuocBinh/shards_norm_v3/shard_*_token.csv

NER:
    gpu_id: 0
    rand_embedding: True
    batch_size: 16
    unk: unk
    char_hidden: 100
    word_hidden: 100
    dropout: 0.5
    epochs: 500
    start_epoch: 0
    caseless: False
    char_dim: 30
    word_dim: 100
    char_layers: 1
    word_layers: 1
    lr: 0.001
    lr_decay: 0.05
    lambda0: 1.0
    fine_tune: True
    momentum: 0.9
    clip_grad: 5.0
    large_crf: True
    mini_count: 5
    co_train: True
    patience: 20
    if_highway: True
    highway_layers: 1
    eva_matrix: fa
    least_iters: 50
    shrink_embedding: False
    use_attention: False
    gpu: -1
    ;save_model_to: ner.model
    ;load_model_from: ner.model
    train_file: train.conll
    dev_file: dev.conll
    test_file: test.conll

    train_file_1: ./data/viettreebank_train.seg.conll
    dev_file_1: ./data/viettreebank_valid.seg.conll
    test_file_1: ./data/viettreebank_test.seg.conll

    train_file_2: ./data/data_end_mini/train_v2.conll
    test_file_2: ./data/data_end_mini/test_v2.conll
    dev_file_2: ./data/data_end_mini/dev_v2.conll

    train_norm: ./data/tts_norm_ner/train.conll
    val_norm: ./data/tts_norm_ner/val.conll
    test_norm: ./data/tts_norm_ner/test.conll

    train_tts: resources/conll/train_tts.conll
    val_tts: resources/conll/val_tts.conll
    test_tts: resources/conll/test_tts.conll

    train_asr: resources/conll/train_asr.conll
    val_asr: resources/conll/val_asr.conll
    test_asr: resources/conll/test_asr.conll

    train_auto_punct_asr: resources/conll_asr/shard_auto_punct_v2_20percent_bieos_500k.train
    val_auto_punct_asr: resources/conll_asr/shard_auto_punct_v2_20percent_bieos_500k.val
    test_auto_punct_asr: resources/conll_asr/shard_auto_punct_v2_20percent_bieos_500k.test

    checkpoint_ner_tts: ner/checkpoint/norm_ner_tts.model
    ;checkpoint_ner_tts: ner/checkpoint/norm_ner_v2.model
    checkpoint_ner_asr: ner/checkpoint/norm_ner_asr.model
    ;checkpoint_ner_auto_punct_asr: ner/checkpoint/norm_ner_auto_punct_asr_30000.model
    checkpoint_ner_auto_punct_asr: ner/checkpoint/norm_ner_auto_punct_v2_20percent_bieos_500k.model
    ;checkpoint_ner_auto_punct_asr: ner/checkpoint/norm_ner_auto_punct_asr_small.model


norm-pattern:
    km_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*km\s
    m_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*m\s
    cm_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*cm\s
    mm_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*mm\s
    nm_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*nm\s
    ha_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*ha\s
    l_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*L\s
    kg_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*kg\s
    g_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*g\s
    gr_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*gram\s
    mg_pattern: \s[0-9]*\.*\,*\-*[0-9]+\s*mg\s

norm-class:
    <PUNCT>: </PUNCT>
    <MEASURE>: </MEASURE>
    <DATE>: </DATE>
    <TIME>: </TIME>
    <VERBATIM>: </VERBATIM>
    <CARDINAL>: </CARDINAL>
    <ROMAN>: </ROMAN>
    <DIGIT>: </DIGIT>
    <DECIMAL>: </DECIMAL>
    <ADDRESS>: </ADDRESS>
    <FRACTION>: </FRACTION>
    <ABBRE>: </ABBRE>
    <FOREIGN>: </FOREIGN>
    <LETTER>: </LETTER>

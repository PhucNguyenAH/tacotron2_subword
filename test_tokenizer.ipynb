{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"căn biệt phủ của ông bùi cách tuyến chỉ rộng 120 m2\"\n",
    "# text = \"+ mỗi vé có một số series và không trùng nhau + có nhiều số series trùng nhau .\\n\"\n",
    "text = \"được biết trước khi làm chủ tịch ubnd huyện côn đảo , ông chính công tác tại bộ chỉ huy quân sự tỉnh bà rịa - vũng tàu .\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MBartTokenizer\n",
    "from text import text_to_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MBartTokenizer.from_pretrained('facebook/mbart-large-cc25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([     2,    912,   4022,   5179,   1907,   1839,   6657,  40443,      6,\n",
      "          5829,   2208,  62633,   4747,     19,  54520,      6,      4,   5718,\n",
      "          3178,   1871,   6330,   2251,   5830,   2524,  58854,  29225,   2550,\n",
      "         17501,  15409,   1690,  51721,     11,     20,     81, 145131,  72894,\n",
      "             6,      5,      2, 250024])\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer(\"</s>\" + text + \"</s> vi_VN\", add_special_tokens=False, return_tensors='pt')['input_ids']\n",
    "print(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['</s>', '▁được', '▁biết', '▁trước', '▁khi', '▁làm', '▁chủ', '▁tịch', '▁', 'ub', 'nd', '▁huyện', '▁cô', 'n', '▁đảo', '▁', ',', '▁ông', '▁chính', '▁công', '▁tác', '▁tại', '▁bộ', '▁chỉ', '▁huy', '▁quân', '▁sự', '▁tỉnh', '▁bà', '▁r', 'ị', 'a', '▁-', '▁v', 'ũng', '▁tàu', '▁', '.', '</s>', 'vi_VN']\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.convert_ids_to_tokens(ids[0])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BARTPho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bartpho-syllable\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    0,    11,    88,   127,    26,    50,   168,   499,    23, 10982,\n",
      "         7148,   435,   200,    19,   738,    23,     4,    69,    74,    25,\n",
      "          139,    34,   123,    68,   891,   298,    54,   223,   329,   339,\n",
      "         1289,    97,    33,   258,  2081,   537,    23,     5,     2])\n"
     ]
    }
   ],
   "source": [
    "ids = tokenizer(text, return_tensors='pt')['input_ids']\n",
    "print(ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '▁được', '▁biết', '▁trước', '▁khi', '▁làm', '▁chủ', '▁tịch', '▁', 'ub', 'nd', '▁huyện', '▁cô', 'n', '▁đảo', '▁', ',', '▁ông', '▁chính', '▁công', '▁tác', '▁tại', '▁bộ', '▁chỉ', '▁huy', '▁quân', '▁sự', '▁tỉnh', '▁bà', '▁r', 'ị', 'a', '▁-', '▁v', 'ũng', '▁tàu', '▁', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.convert_ids_to_tokens(ids[0])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PhoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from text import text_to_sequence, sequence_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-large\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0,    11,    55,    71,    26,    47,   286, 26865, 29494, 26189,\n",
       "          149, 12840,   705,     4,    46,   159,   675, 18116,    35,   215,\n",
       "           66, 23250,   829,    61,    98,   155,  1698, 40687,    31, 11953,\n",
       "          356,     5,     2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = torch.tensor([tokenizer.encode(text)])\n",
    "ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'được', 'biết', 'trước', 'khi', 'làm', 'chủ', 'tịch', 'ub@@', 'nd', 'huyện', 'côn', 'đảo', ',', 'ông', 'chính', 'công', 'tác', 'tại', 'bộ', 'chỉ', 'huy', 'quân', 'sự', 'tỉnh', 'bà', 'r@@', 'ịa', '-', 'vũng', 'tàu', '.', '</s>']\n"
     ]
    }
   ],
   "source": [
    "token = tokenizer.convert_ids_to_tokens(ids[0])\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"When asked to explain the similarity of characteristics, Cadigan stated, quote, well briefly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cls_sep(text):\n",
    "    return \"[CLS] \" + text + \" [SEP]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 48, 34, 59, 11, 12, 81, 56, 34, 32, 11, 82, 60, 11, 34, 97, 78, 57, 12, 49, 59, 11, 82, 48, 34, 11, 81, 49, 58, 49, 57, 12, 80, 49, 82, 98, 11, 60, 46, 11, 31, 48, 12, 80, 12, 31, 82, 34, 80, 49, 81, 82, 49, 31, 81, 6, 11, 31, 12, 32, 49, 47, 12, 59, 11, 81, 82, 12, 82, 34, 32, 6, 11, 79, 83, 60, 82, 34, 6, 11, 96, 34, 57, 57, 11, 30, 80, 49, 34, 46, 57, 98]\n"
     ]
    }
   ],
   "source": [
    "s = text_to_sequence(text, [\"english_cleaners\"])\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when asked to explain the similarity of characteristics, cadigan stated, quote, well briefly\n"
     ]
    }
   ],
   "source": [
    "text_cleaned = sequence_to_text(s)\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT \n",
    "### HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from text import text_to_sequence, sequence_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2043,  2356,  2000,  4863,  1996, 14402,  1997,  6459,  1010,\n",
      "         28353, 10762,  3090,  1010, 14686,  1010,  2092,  4780,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "text_processed = add_cls_sep(text_cleaned)\n",
    "tokenized_text = tokenizer.tokenize(text_processed)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0 for i in range(len(indexed_tokens))]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 768])\n",
      "-----------------\n",
      "torch.Size([1, 768])\n",
      "tensor([[[-0.0352,  0.2057, -0.6240,  ..., -0.1726,  0.4053,  0.3813],\n",
      "         [-0.2486,  0.0666, -0.5241,  ...,  0.3820,  0.3803, -0.0616],\n",
      "         [-0.0244, -0.4694, -0.1529,  ...,  0.3105, -0.0269, -0.3088],\n",
      "         ...,\n",
      "         [ 0.5606,  0.3519,  0.5786,  ...,  0.1516, -0.2715, -0.1949],\n",
      "         [-0.1893, -0.0686, -0.4035,  ...,  0.7745,  0.1391, -0.1611],\n",
      "         [ 0.7732,  0.3311, -0.3702,  ...,  0.6668, -0.8738, -0.3741]]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_bert(tokens_tensor, segments_tensors)[0][0].shape)\n",
    "    print(\"-----------------\")\n",
    "    print(model_bert(tokens_tensor, segments_tensors)[1].shape)\n",
    "    encoded_layers = model_bert(tokens_tensor, segments_tensors)[0]\n",
    "    print(encoded_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT \n",
    "### pytorch_pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  2043,  2356,  2000,  4863,  1996, 14402,  1997,  6459,  1010,\n",
      "         28353, 10762,  3090,  1010, 14686,  1010,  2092,  4780,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "text_processed = add_cls_sep(text_cleaned)\n",
    "tokenized_text = tokenizer.tokenize(text_processed)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0 for i in range(len(indexed_tokens))]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([19, 768])\n",
      "--------------\n",
      "torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_bert(tokens_tensor, segments_tensors)[0][11][0].shape)\n",
    "    print(\"--------------\")\n",
    "    print(model_bert(tokens_tensor, segments_tensors)[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT \n",
    "### multilingual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = \"When asked to explain the similarity of characteristics, Cadigan stated, quote, well briefly\"\n",
    "text_vi = \"được biết trước khi làm chủ tịch ubnd huyện côn đảo , ông chính công tác tại bộ chỉ huy quân sự tỉnh bà rịa - vũng tàu .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from text import text_to_sequence, sequence_to_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 732kB/s]\n",
      "Downloading: 100%|██████████| 681M/681M [01:31<00:00, 7.83MB/s] \n",
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 972k/972k [00:06<00:00, 144kB/s] \n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 25.2kB/s]\n",
      "Downloading: 100%|██████████| 1.87M/1.87M [00:05<00:00, 335kB/s] \n"
     ]
    }
   ],
   "source": [
    "model_bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_cls_sep(text):\n",
    "    return \"[CLS] \" + text + \" [SEP]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vietnam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33, 89, 77, 31, 11, 30, 49, 41, 82, 11, 82, 80, 89, 73, 31, 11, 56, 48, 49, 11, 57, 15, 58, 11, 31, 48, 85, 11, 82, 54, 31, 48, 11, 83, 30, 59, 32, 11, 48, 83, 98, 45, 59, 11, 31, 66, 59, 11, 33, 14, 60, 11, 6, 11, 66, 59, 47, 11, 31, 48, 50, 59, 48, 11, 31, 66, 59, 47, 11, 82, 13, 31, 11, 82, 17, 49, 11, 30, 71, 11, 31, 48, 51, 11, 48, 83, 98, 11, 79, 83, 18, 59, 11, 81, 94, 11, 82, 51, 59, 48, 11, 30, 15, 11, 80, 54, 12, 11, 1, 11, 95, 87, 59, 47, 11, 82, 15, 83, 11, 7]\n",
      "được biết trước khi làm chủ tịch ubnd huyện côn đảo , ông chính công tác tại bộ chỉ huy quân sự tỉnh bà rịa - vũng tàu .\n"
     ]
    }
   ],
   "source": [
    "s = text_to_sequence(text_vi, [\"basic_cleaners\"])\n",
    "print(s)\n",
    "text_cleaned = sequence_to_text(s)\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'được', 'biết', 'trước', 'khi', 'làm', 'chủ', 'tịch', 'u', '##bn', '##d', 'huyện', 'côn', 'đảo', ',', 'ông', 'chính', 'công', 'tác', 'tại', 'bộ', 'chỉ', 'huy', 'quân', 'sự', 'tỉnh', 'bà', 'r', '##ị', '##a', '-', 'vũ', '##ng', 'tàu', '.', '[SEP]']\n",
      "tensor([[  101, 10476, 21820, 16325, 12072, 12984, 16549, 41946,   189, 71136,\n",
      "         10162, 15222, 84547, 22587,   117, 12660, 12707, 12319, 17976, 12086,\n",
      "         13848, 14294, 33901, 12488, 12636, 14221, 27083,   186, 30324, 10113,\n",
      "           118, 36108, 10376, 19415,   119,   102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "text_processed = add_cls_sep(text_cleaned)\n",
    "tokenized_text = tokenizer.tokenize(text_processed)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0 for i in range(len(indexed_tokens))]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokenized_text)\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[96, 48, 34, 59, 11, 12, 81, 56, 34, 32, 11, 82, 60, 11, 34, 97, 78, 57, 12, 49, 59, 11, 82, 48, 34, 11, 81, 49, 58, 49, 57, 12, 80, 49, 82, 98, 11, 60, 46, 11, 31, 48, 12, 80, 12, 31, 82, 34, 80, 49, 81, 82, 49, 31, 81, 6, 11, 31, 12, 32, 49, 47, 12, 59, 11, 81, 82, 12, 82, 34, 32, 6, 11, 79, 83, 60, 82, 34, 6, 11, 96, 34, 57, 57, 11, 30, 80, 49, 34, 46, 57, 98]\n",
      "when asked to explain the similarity of characteristics, cadigan stated, quote, well briefly\n"
     ]
    }
   ],
   "source": [
    "s = text_to_sequence(text_en, [\"basic_cleaners\"])\n",
    "print(s)\n",
    "text_cleaned = sequence_to_text(s)\n",
    "print(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'when', 'asked', 'to', 'explain', 'the', 'similar', '##ity', 'of', 'characteristics', ',', 'ca', '##digan', 'stated', ',', 'quo', '##te', ',', 'well', 'briefly', '[SEP]']\n",
      "tensor([[  101, 10841, 22151, 10114, 67004, 10105, 13213, 11949, 10108, 40582,\n",
      "           117, 11135, 66637, 17067,   117, 48718, 10216,   117, 11206, 36327,\n",
      "           102]])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "text_processed = add_cls_sep(text_cleaned)\n",
    "tokenized_text = tokenizer.tokenize(text_processed)\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "segments_ids = [0 for i in range(len(indexed_tokens))]\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "print(tokenized_text)\n",
    "print(tokens_tensor)\n",
    "print(segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
